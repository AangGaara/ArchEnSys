col.hidden ="red",
col.out    ="blue",
show.weights = FALSE,
information.pos = 20)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = FALSE,
information.pos = 1)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = FALSE,
information = FALSE)
#nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
#                data = train_dataset,
#                hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
#                act.fct = "logistic",
#                linear.output = TRUE)
nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
data = train_dataset,
hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
stepmax = 1e+5,
act.fct = "logistic",
linear.output = TRUE)
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# nice to read: https://datascienceplus.com/fitting-neural-network-in-r/
# from https://www.youtube.com/watch?v=Eecg_Nt8LLc
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training & testing data set
# 80% of the data will be used
train_obs_size = ceiling(nrow(nn_dataset) * 0.8) #80% of the data will be used for training
test_obs_size  = nrow(nn_dataset) - train_obs_size
train_dataset   <- nn_dataset[1:train_obs_size, ]
test_dataset    <- nn_dataset[(train_obs_size+1):nrow(nn_dataset),]
# fit neural network
number_of_variables <- ncol(train_dataset)
#nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
#                data = train_dataset,
#                hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
#                act.fct = "logistic",
#                linear.output = TRUE)
nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
data = train_dataset,
hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
stepmax = 1e+6,
act.fct = "logistic",
linear.output = TRUE)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = FALSE)
#prediction using NN
Predict = neuralnet::compute(nn, test_dataset)
results <- data.frame(actual = test_dataset$dist_gwh, prediction = Predict$net.result)
results
#accuracy
#from https://datascienceplus.com/neuralnet-train-and-test-neural-networks-using-r/
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = FALSE,
fontsize = 5)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = FALSE,
fontsize = 7.5)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = FALSE,
fontsize = 10)
#nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
#                data = train_dataset,
#                hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
#                act.fct = "logistic",
#                linear.output = TRUE)
nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
data = train_dataset,
hidden = c(ncol(train_dataset), ncol(train_dataset)),
stepmax = 1e+6,
act.fct = "logistic",
linear.output = TRUE)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = FALSE,
fontsize = 10)
#prediction using NN
Predict = neuralnet::compute(nn, test_dataset)
results <- data.frame(actual = test_dataset$dist_gwh, prediction = Predict$net.result)
results
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information = TRUE,
fontsize = 10)
plot$information
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information.pos = 0.1,
fontsize = 10)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information.pos = 100,
fontsize = 10)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information.pos = 1250,
fontsize = 10)
nn$weights
nn$act.fct
nn$call
nn$model.list
nn$covariate
nn
nn$net.result
nn$result.matrix
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
nn_dataset$Province <- NULL
View(nn_dataset)
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
View(nn_dataset)
# max-min normalization
preproc_dataset <- preProcess(nn_dataset[,c(1,3:10)], method=c("range"))
normalize(nn_dataset[,c(1,3:10)])
install.packages("normalr")
# max-min normalization
getLambda(nn_dataset, parallel = FALSE)
library(normalr)
# max-min normalization
getLambda(nn_dataset, parallel = FALSE)
library(normalr)
getLambda(nn_dataset[,c(1,3:10)], parallel = FALSE)
MTCARS
mtcars
getLambda(mtcars, parallel = FALSE)
getLambda(nn_dataset, parallel = FALSE)
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
normalize
x <- nn_dataset()
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
x <- nn_dataset()
x <- nn_dataset
normalize(x)
a <- normalize(x)
a
x <- nn_dataset[,c(1,3:10)]
a <- normalize(x)
a
View(a)
return ((x - min(x)) / (max(x) - min(x)))}
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
nn_dataset$Province <- NULL
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset, normalize))
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset, normalize))
nn_dataset$Province <- NULL
nn_dataset$Region <- NULL
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset, normalize))
maxmindf
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:10)], normalize))
maxmindf
View(maxmindf)
source("demand_forecast_data_source.R")
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:10)], normalize))
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
View(nn_dataset)
# max-min normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:10)], normalize))
View(maxmindf)
max_val <- max(nn_dataset[,c(1,3:10))
max_val <- max(nn_dataset[,c(1,3:10)])
max_val
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
colMax(n_dataset)
return((x - min(x)) / (max(x) - min(x)))}
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:10)], normalize))
max_val <- colMax(nn_dataset[,c(1,3:10)])
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
max_val <- colMax(nn_dataset[,c(1,3:10)])
max_val
View(nn_dataset)
max_val <- colMax(nn_dataset[,c(1,3:11)])
min_val <- colMin(nn_dataset[,c(1,3:11)])
scale_val <- max_val - min_val
scale_val
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
max_val <- colMax(nn_dataset[,c(1,3:11)])
min_val <- colMin(nn_dataset[,c(1,3:11)])
scale_val <- max_val - min_val
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:11)], normalize))
View(maxmindf)
max_val
min_val
original_dat <- as.data.frame(lapply(maxmindf, normalize))
View(original_dat)
original_dat <- as.data.frame(lapply(maxmindf, de_normalize))
return((max(x)-min(x))/(x - min(x)))}
return((max(x) - min(x)) / (x - min(x))}
original_dat <- as.data.frame(lapply(maxmindf, de_normalize))
# Create training & testing data set
# 80% of the data will be used
train_obs_size = ceiling(nrow(maxmindf) * 0.8) #80% of the data will be used for training
test_obs_size  = nrow(maxmindf) - train_obs_size
train_dataset   <- maxmindf[1:train_obs_size, ]
test_dataset    <- maxmindf[(train_obs_size+1):nrow(maxmindf),]
# fit neural network
number_of_variables <- ncol(train_dataset)
#nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
#                data = train_dataset,
#                hidden = c(ncol(train_dataset), ncol(train_dataset), ncol(train_dataset), ncol(train_dataset)),
#                act.fct = "logistic",
#                linear.output = TRUE)
nn <- neuralnet(dist_gwh ~ year + gdpr_billion_idr + gdpr_growth + population + intensity_biased,
data = train_dataset,
hidden = c(ncol(train_dataset), ncol(train_dataset)),
stepmax = 1e+6,
act.fct = "logistic",
linear.output = TRUE)
plot(nn, rep = "best",
arrow.length = 0.15,
col.entry  ="blue",
col.hidden ="red",
col.out    ="blue",
show.weights = TRUE,
information  = TRUE,
fontsize = 10)
#prediction using NN
Predict = neuralnet::compute(nn, test_dataset)
results <- data.frame(actual = test_dataset$dist_gwh, prediction = Predict$net.result)
results
return((max(x) - min(x)) / (x - min(x)))}
return((max(x) - min(x)) / (x - min(x)))}
return((y*(max_val - min_val)) + min_val)
return((x*(max_val - min_val)) + min_val)
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))}
return((z*(max_val - min_val)) + min_val)
# de-normalization
de_normalize <- function(z) {
return((z*(max_val - min_val)) + min_val)
}
return((z*(max_val - min_val)) + min_val)}
return((z*(max_val - min_val)) + min_val)
# de-normalization
de_normalize <- function(z) {
return((z*(max_val - min_val)) + min_val)
}
nornor <- s.data.frame(lapply(maxmindf, de_normalize))
nornor <- as.data.frame(lapply(maxmindf, de_normalize))
View(nornor)
nornor <- as.data.frame(lapply(maxmindf[,c(1:10)], de_normalize))
View(nornor)
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
library(neuralnet)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(normalr)
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
install.packages('gradDescent')
# max-min normalization
aass <- minmaxScaling(nn_dataset)
library(gradDescent)
# max-min normalization
aass <- minmaxScaling(nn_dataset)
# max-min normalization
aass <- minmaxScaling(nn_dataset[,c(1,3:11)])
View(aass)
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
max_val <- colMax(nn_dataset[,c(1,3:11)])
min_val <- colMin(nn_dataset[,c(1,3:11)])
scale_val <- max_val - min_val
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:11)], normalize))
maxmindf <- (maxmindf*scale_val) + min_val
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
max_val <- colMax(nn_dataset[,c(1,3:11)])
min_val <- colMin(nn_dataset[,c(1,3:11)])
scale_val <- max_val - min_val
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:11)], normalize))
setwd("C:/Users/Brajamusthi/OneDrive/Master of Energy Research/Github/ArchEnSys")
source("demand_forecast_data_source.R")
#nn_dataset <- historical_data_source
#remove indonesia
nn_dataset <- historical_data_source[historical_data_source$Province =="INDONESIA",]
# call min and max value for each column
colMax <- function(data) sapply(data, max, na.rm = TRUE)
colMin <- function(data) sapply(data, min, na.rm = TRUE)
max_val <- colMax(nn_dataset[,c(1,3:11)])
min_val <- colMin(nn_dataset[,c(1,3:11)])
scale_val <- max_val - min_val
# max-min normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(nn_dataset[,c(1,3:11)], normalize))
ori_value<- (maxmindf*scale_val) + min_val
View(ori_value)
a <- c(1,2,3,4,5)
a <- c(1,1,1,1,1)
b <- c(2,2,2,2,2)
x <- data.frame(a,b)
x
c <- c(3,3,3,3,3)
x <- data.frame(a,b)
x <- data.frame(a,b,c)
x
y <- x*a
y
y <- x*b
y
a <- c(1,1,1,1,4)
b <- c(2,2,2,2,4)
c <- c(3,3,3,3,4)
x <- data.frame(a,b,c)
x
y <- x*b
y
y <- x%*%b
y <- x*d
d <- c(5,5,5)
y <- x*d
y
a <- c(1,1,1,1,4)
b <- c(2,2,2,2,4)
c <- c(3,3,3,3,4)
d <- c(5,5,5)
x <- data.frame(a,b,c)
x
y <- x*d
y
a <- c(1,1,1,1,1)
b <- c(2,2,2,2,2)
c <- c(1,1,1,1,1)
d <- c(5,5,5)
x <- data.frame(a,b,c)
x
y <- x*d
y
y <- x*b
y
a <- c(1,1,1,1,1)
b <- c(2,2,2,2,2)
c <- c(1,1,1,1,1)
d <- c(5,5,5)
x <- data.frame(a,b,c)
x
y <- x+a
y
y <- x+b
y
ori_value <- (maxmindf*scale_val) + min_val
