names(capacity_2011_2012)[1] <- "Province"
capacity_2011_2012 <- pivot_longer(capacity_2011_2012, cols = 2:3, names_to = "year", values_to = "capacity_mw")
capacity_2013_2015 <- read_excel("data_demand_forecast/capacity/capacity_mw_2013_2015.xlsx", range = "A2:D37", col_names = TRUE)
names(capacity_2013_2015)[1] <- "Province"
capacity_2013_2015 <- pivot_longer(capacity_2013_2015, cols = 2:4, names_to = "year", values_to = "capacity_mw")
capacity_2017_2019 <- read_excel("data_demand_forecast/capacity/capacity_mw_2017_2019.xlsx", range = "A2:D37", col_names = TRUE)
names(capacity_2017_2019)[1] <- "Province"
capacity_2017_2019 <- pivot_longer(capacity_2017_2019, cols = 2:4, names_to = "year", values_to = "capacity_mw")
#merge all data
capacity_2011_2019 <- rbind(capacity_2011_2012,
capacity_2013_2015,
capacity_2017_2019)
#remove all spaces, and convert comma to dot, replace "_" with 0
capacity_2011_2019[3]          <- sub(",", ".", capacity_2011_2019$capacity_mw, fixed = TRUE)
capacity_2011_2019[3]          <- sub("-", "0", capacity_2011_2019$capacity_mw, fixed = TRUE)
capacity_2011_2019             <- capacity_2011_2019 %>% mutate(capacity_mw = str_remove(capacity_mw, "\\s"))
capacity_2011_2019$capacity_mw <- as.numeric (capacity_2011_2019$capacity_mw)
# Bind all data ---------------------------------------------------------------------
historical_data_source <- merge(gdpr_2010_2020,         gdpr_percap_2010_2020, by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, gdpr_growth_2011_2020, by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, population_data,       by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, gen_gwh_2011_2019,     by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, dist_gwh_2011_2019,    by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, gwh_consumption_percap,by = c("Province","year"), sort = FALSE)
historical_data_source <- merge(historical_data_source, capacity_2011_2019,    By = c("Province","year"), sort = FALSE)
# assign energy intensity to the main data
energy_intensity[3:14] <- NULL
energy_intensity[1]    <- NULL
historical_data_source <- merge(historical_data_source, energy_intensity, all.x = TRUE, by = "year", sort  = FALSE)
# set year as numeric
historical_data_source$year <- as.numeric(historical_data_source$year)
# replace or NA with 0 (zero)
historical_data_source[is.na(historical_data_source)] <- 0
# add region column
#historical_data_source <- transform(historical_data_source, Region = historical_data_source$Province)
historical_data_source <- within(historical_data_source, {
#create blank region
Region <- NA
#Sumatera region
Region[Province == "ACEH"]                  <- Region_1
Region[Province == "SUMATERA UTARA"]        <- Region_1
Region[Province == "SUMATERA BARAT"]        <- Region_1
Region[Province == "RIAU"]                  <- Region_1
Region[Province == "JAMBI"]                 <- Region_1
Region[Province == "SUMATERA SELATAN"]      <- Region_1
Region[Province == "BENGKULU"]              <- Region_1
Region[Province == "LAMPUNG"]               <- Region_1
Region[Province == "KEP. BANGKA BELITUNG"]  <- Region_1
Region[Province == "KEP. RIAU"]             <- Region_1
#Java region
Region[Province == "DKI JAKARTA"]   <- Region_7
Region[Province == "JAWA BARAT"]    <- Region_7
Region[Province == "JAWA TENGAH"]   <- Region_7
Region[Province == "DI YOGYAKARTA"] <- Region_7
Region[Province == "JAWA TIMUR"]    <- Region_7
Region[Province == "BANTEN"]        <- Region_7
#Bali & Nusa Tenggara region
Region[Province == "BALI"]                  <- Region_6
Region[Province == "NUSA TENGGARA BARAT"]   <- Region_6
Region[Province == "NUSA TENGGARA TIMUR"]   <- Region_6
#Kalimantan region
Region[Province == "KALIMANTAN BARAT"]      <- Region_2
Region[Province == "KALIMANTAN TENGAH"]     <- Region_2
Region[Province == "KALIMANTAN SELATAN"]    <- Region_2
Region[Province == "KALIMANTAN TIMUR"]      <- Region_2
Region[Province == "KALIMANTAN UTARA"]      <- Region_2
#Sulawesi region
Region[Province == "SULAWESI UTARA"]        <- Region_3
Region[Province == "SULAWESI TENGAH"]       <- Region_3
Region[Province == "SULAWESI SELATAN"]      <- Region_3
Region[Province == "SULAWESI TENGGARA"]     <- Region_3
Region[Province == "GORONTALO"]             <- Region_3
Region[Province == "SULAWESI BARAT"]        <- Region_3
#Maluku region
Region[Province == "MALUKU"]       <- Region_4
Region[Province == "MALUKU UTARA"] <- Region_4
#Papua region
Region[Province == "PAPUA BARAT"]  <- Region_5
Region[Province == "PAPUA"]        <- Region_5
#National region
Region[Province == "INDONESIA"]    <- Region_8
})
# End ---------------------------------------------------------------------
# write data
# write_csv(historical_data_source, "data_demand_forecast/forecast_data_source.csv") #whole data
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr_growth,
data = training_dataset,
hidden = number_of_variables,
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
# Crete test set
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr_growth,
data = training_dataset,
hidden = number_of_variables,
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = number_of_variables,
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
#coba create trainint data set
TKS = c(20,10,30,20,80,30)
CSS = c(90,20,40,50,50,80)
Placed = c(1,0,0,0,1,1)
df = data.frame(TKS, CSS, Placed)
#fit neural netowrk
nn = neuralnet(Placed~TKS+CSS, data = df, hidden = 3, act.fct = "logistic", linear.output = FALSE)
plot(nn)
# create test set
TKS = c(30,40,85)
CSS = c(85,50,40)
test = data.frame(TKS, CSS)
#Prediction using neural network
Predict = compute(nn, test)
Predict$net.result
#convert probabilities into binary classes setting threshold level 0.5
prob <- Predict$net.result
pred <- ifelse(prob>0.5, 1, 0)
pred
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = number_of_variables,
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
# Crete test set
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
plot(neural_network)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
plot(neural_network)
# Crete test set
year         <- historical_data_source$year[training_variables, row_number]
# Crete test set
year         <- historical_data_source$year[training_variable:, row_number]
# Crete test set
year         <- historical_data_source$year[training_variable: row_number]
# Crete test set
year         <- historical_data_source$year[training_variables:row_number]
# Crete test set
year         <- historical_data_source$year[training_variables:row_number]
gdpr         <- historical_data_source$gdpr_billion_idr[training_variables:row_number]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[training_variables:row_number]
population   <- historical_data_source$population[training_variables:row_number]
intensity    <- historical_data_source$intensity_biased[training_variables:row_number]
gwh_dist     <- historical_data_source$dist_gwh[training_variables:row_number]
testing_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
#prediction using NN
Predict = compute(neural_network, testing_dataset)
Predict$net.result
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
# Crete test set
year         <- historical_data_source$year[training_variables:row_number]
gdpr         <- historical_data_source$gdpr_billion_idr[training_variables:row_number]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[training_variables:row_number]
population   <- historical_data_source$population[training_variables:row_number]
intensity    <- historical_data_source$intensity_biased[training_variables:row_number]
gwh_dist     <- historical_data_source$dist_gwh[training_variables:row_number]
testing_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
#prediction using NN
Predict = compute(neural_network, testing_dataset)
Predict$net.result
#prediction using NN
Predict = compute(neural_network, testing_dataset)
Predict$all.units
Predict$net.result
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
neural_network
# Crete test set
year         <- historical_data_source$year[training_variables-1:row_number]
gdpr         <- historical_data_source$gdpr_billion_idr[training_variables-1:row_number]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[training_variables-1:row_number]
year         <- historical_data_source$year[training_variables:row_number]
gdpr         <- historical_data_source$gdpr_billion_idr[training_variables:row_number]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[training_variables:row_number]
population   <- historical_data_source$population[training_variables:row_number]
intensity    <- historical_data_source$intensity_biased[training_variables:row_number]
gwh_dist     <- historical_data_source$dist_gwh[training_variables:row_number]
testing_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
#prediction using NN
Predict = compute(neural_network, testing_dataset)
Predict$net.result
plot(neural_network)
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
library(neuralnet)
library(tidyverse)
source("demand_forecast_data_source.R")
#main steps
# from: https://www.datacamp.com/community/tutorials/neural-network-models-r
# 1. Create training data set
# 2. FIt neural network
# 3. Create test set
# 4. Prediction using Neural Network
# 5. Call the trained netowrk
# Create training data set
# 80% of the data will be used
training_data_percent = 0.8 #80% of the data will be used for training
test_data_percent = 1 - training_data_percent #the rest of the data is for testing
row_number = nrow(historical_data_source)
training_variables = ceiling(row_number * training_data_percent)
year         <- historical_data_source$year[1:training_variables]
gdpr         <- historical_data_source$gdpr_billion_idr[1:training_variables]
gdpr_growth  <- historical_data_source$`gdpr_growth_%`[1:training_variables]
population   <- historical_data_source$population[1:training_variables]
intensity    <- historical_data_source$intensity_biased[1:training_variables]
gwh_dist     <- historical_data_source$dist_gwh[1:training_variables]
training_dataset   <- data.frame(year,
gdpr,
gdpr_growth,
population,
intensity,
gwh_dist)
# fit neural network
number_of_variables <- ncol(training_dataset)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables, number_of_variables),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = number_of_variables,
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
neural_network = neuralnet(gwh_dist ~ year + gdpr + gdpr_growth + population + intensity,
data = training_dataset,
hidden = c(number_of_variables, number_of_variables - 1),
act.fct = "logistic",
linear.output = FALSE)
plot(neural_network)
